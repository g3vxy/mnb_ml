{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('./dataset.csv', \n",
    "                      header=None, \n",
    "                      names=[\"Comment\", \"Movie\" ,\"Bool\"])\n",
    "\n",
    "dataset = dataset.tail(-1)\n",
    "dataset = dataset.drop(\"Movie\", 1)\n",
    "\n",
    "dataset = dataset.sample(frac=.008, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_points(x):\n",
    "    x = float(x.replace(',', '.'))\n",
    "    if x < 3.2:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def clean_values(x):\n",
    "    x = x.replace('\\n', ' ')\n",
    "    x = x.replace('\\W', ' ')\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "dataset['Bool'] = dataset['Bool'].apply(map_points)\n",
    "dataset['Comment'] = dataset['Comment'].apply(clean_values)\n",
    "dataset['Comment'] = dataset['Comment'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for comment in dataset['Comment']:\n",
    "    for word in comment:\n",
    "        vocabulary.append(word)\n",
    "# zayıflıklardan biri datasetin temizlenmesine çok bağlı\n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "# datanın temizlenmesi ve sözlüğün oluşması burada bitiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frekans tablosu oluşturuyoruz\n",
    "word_counts_per_comment = {unique_word: [0] * len(dataset['Comment']) for unique_word in vocabulary}\n",
    "\n",
    "for index, comment in enumerate(dataset['Comment']):\n",
    "    for word in comment:\n",
    "        word_counts_per_comment[word][index] += 1\n",
    "\n",
    "word_counts = pd.DataFrame(word_counts_per_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_joined = pd.concat([dataset, word_counts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "positive_values = dataset_joined[dataset_joined['Bool'] == np.bool_(True)]\n",
    "negative_values = dataset_joined[dataset_joined['Bool'] == np.bool_(False)]\n",
    "\n",
    "positive_percentage = positive_values.shape[0] / len(dataset_joined)\n",
    "negative_percentage = negative_values.shape[0] / len(dataset_joined)\n",
    "\n",
    "n_words_per_positive_message = positive_values['Comment'].apply(len)\n",
    "n_positive = n_words_per_positive_message.sum()\n",
    "\n",
    "n_words_per_negative_message = negative_values['Comment'].apply(len)\n",
    "n_negative = n_words_per_negative_message.sum()\n",
    "\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "alpha = 1 # laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_positive = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_negative = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_given_positive = positive_values[word].sum()\n",
    "    p_word_given_positive = (n_word_given_positive + alpha) / (n_positive + alpha*n_vocabulary)\n",
    "    parameters_positive[word] = p_word_given_positive\n",
    "\n",
    "    n_word_given_negative = negative_values[word].sum()\n",
    "    p_word_given_negative = (n_word_given_negative + alpha) / (n_negative + alpha*n_vocabulary)\n",
    "    parameters_negative[word] = p_word_given_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a2e680c6d30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mWPT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Yukarıdaki rowları normalize etme işlemine de bu method uygulanabilir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "# Yukarıdaki rowları normalize etme işlemine de bu method uygulanabilir\n",
    "# bir ara implente et.\n",
    "def norm_doc(single_doc):\n",
    "    # TR: Dokümandan belirlenen özel karakterleri ve sayıları at\n",
    "    # EN: Remove special characters and numbers\n",
    "    single_doc = re.sub(\" \\d+\", \" \", single_doc)\n",
    "    pattern = r\"[{}]\".format(\",.;\") \n",
    "    single_doc = re.sub(pattern, \"\", single_doc) \n",
    "    # TR: Dokümanı küçük harflere çevir\n",
    "    # EN: Convert document to lowercase\n",
    "    single_doc = single_doc.lower()\n",
    "    single_doc = single_doc.strip()\n",
    "    # TR: Dokümanı token'larına ayır\n",
    "    # EN: Tokenize documents\n",
    "    tokens = WPT.tokenize(single_doc)\n",
    "    # TR: Stop-word listesindeki kelimeler hariç al\n",
    "    # EN: Filter out the stop-words \n",
    "    filtered_tokens = [token for token in tokens if token not in stopwordsarray]\n",
    "    # TR: Dokümanı tekrar oluştur\n",
    "    # EN: Reconstruct the document\n",
    "    single_doc = ' '.join(filtered_tokens)\n",
    "    return single_doc\n",
    "\n",
    "def norm_values(perc_positive, perc_negative):\n",
    "    sum = perc_positive + perc_negative\n",
    "    norm_positive = perc_positive / sum\n",
    "    norm_negative = perc_negative / sum\n",
    "    \n",
    "    return norm_positive, norm_negative\n",
    "\n",
    "def classify(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = norm_doc(message).split()\n",
    "\n",
    "    p_positive_given_message = p_positive\n",
    "    p_negative_given_message = p_negative\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_positive:\n",
    "            p_positive_given_message *= parameters_positive[word]\n",
    "            \n",
    "        if word in parameters_negative:\n",
    "            p_negative_given_message *= parameters_negative[word]\n",
    "    \n",
    "    if p_negative_given_message < p_positive_given_message:\n",
    "        return True, p_positive_given_message, p_negative_given_message\n",
    "    elif p_negative_given_message > p_positive_given_message:\n",
    "        return False, p_positive_given_message, p_negative_given_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Positive|message): 7.244760705484382e-11\n",
      "P(Negative|message): 1.1626534306845597e-12\n",
      "Label: Positive\n"
     ]
    }
   ],
   "source": [
    "classify(\"çok iğrenç bir resim ama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
